{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a9fcdd",
   "metadata": {},
   "source": [
    "![Degirum banner](https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/degirum_banner.png)\n",
    "# Face Recognition & Tracking Tutorials\n",
    "This notebook has two tutorials:\n",
    "1. How to perform face recognition tasks step by step\n",
    "2. How to perform real-time face tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd8f8c",
   "metadata": {},
   "source": [
    "Prerequisites (uncomment to execute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20408aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install degirum-face package\n",
    "# %pip install degirum_face\n",
    "\n",
    "# create cloud API access token\n",
    "# !degirum token create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b55b4b",
   "metadata": {},
   "source": [
    "### Face Recognition Tutorial\n",
    "\n",
    "Define all necessary objects:\n",
    "- face embeddings database\n",
    "- face recognition configuration\n",
    "- create face recognition object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum_face, degirum_tools, numpy as np\n",
    "\n",
    "hardware_to_use = \"N2X/ORCA1\"  # hardware to use for model inference\n",
    "\n",
    "# create face embeddings database control object\n",
    "db = degirum_face.ReID_Database(\"temp/tutorial_db.lance\")  # path to the database files\n",
    "\n",
    "# define model specs to use\n",
    "face_detection_model = (\n",
    "    degirum_face.model_registry.for_task(\"face_detection\")\n",
    "    .for_hardware(hardware_to_use)\n",
    "    .top_model_spec()\n",
    ")\n",
    "face_embedding_model = (\n",
    "    degirum_face.model_registry.for_task(\"face_embedding\")\n",
    "    .for_hardware(hardware_to_use)\n",
    "    .top_model_spec()\n",
    ")\n",
    "\n",
    "# define face recognition configuration\n",
    "face_recognition_config = degirum_face.FaceRecognitionConfig(\n",
    "    face_detection_model=face_detection_model,\n",
    "    face_embedding_model=face_embedding_model,\n",
    "    db=db,\n",
    ")\n",
    "\n",
    "# create FaceRecognition instance: this is the main object to use for face recognition\n",
    "face_recognition = degirum_face.FaceRecognition(face_recognition_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cff169",
   "metadata": {},
   "source": [
    "Define helper function to display images and videos in Jupyter notebooks\n",
    "(this is just a set of service functions for convenience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, Video, display\n",
    "import cv2, tempfile\n",
    "\n",
    "\n",
    "def image_display(img):\n",
    "    display(Image(data=cv2.imencode(\".png\", img)[1].tobytes()))\n",
    "\n",
    "\n",
    "def video_display(video_bytes):\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        video_path = f\"{tmpdir}/video.mp4\"\n",
    "        with open(video_path, \"wb\") as f:\n",
    "            f.write(video_bytes)\n",
    "\n",
    "        display(Video(filename=video_path, width=640, height=480, embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4946440",
   "metadata": {},
   "source": [
    "Enroll persons in the database by analyzing their face images to extract and store face embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd5363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear all tables to start fresh\n",
    "db.clear_all_tables()\n",
    "\n",
    "# enroll Alice and Bob\n",
    "enrolled = face_recognition.enroll_batch(\n",
    "    (\"assets/Alice-1.png\", \"assets/Bob-1.png\"), (\"Alice\", \"Bob\")\n",
    ")\n",
    "\n",
    "# show number of enrolled embeddings in database\n",
    "print(db.count_embeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43adb9b9",
   "metadata": {},
   "source": [
    "Recognize enrolled persons using their other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over other images\n",
    "recognized = []\n",
    "for result in face_recognition.recognize_batch(\n",
    "    (\n",
    "        \"assets/Alice-2.png\",\n",
    "        \"assets/Alice-3.png\",\n",
    "        \"assets/Bob-2.png\",\n",
    "        \"assets/Bob-3.png\",\n",
    "        \"assets/Alice&Bob.png\",\n",
    "    )\n",
    "):\n",
    "    # print recognition results\n",
    "    print(f\"\\nImage '{result.info}'---------------\")\n",
    "    for n, face in enumerate(result.results):\n",
    "        face_result = degirum_face.FaceRecognitionResult.from_dict(face)\n",
    "        print(f\"\\nFace #{n}:\\n{face_result}\")\n",
    "        recognized.append(face_result.embeddings[0])\n",
    "\n",
    "    # display image overlay\n",
    "    image_display(result.image_overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86361a3",
   "metadata": {},
   "source": [
    "Compute pairwise cosine similarities over face embeddings (just for demo purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc92bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity between every pair (recognized, enrolled)\n",
    "sim_matrix = cosine_similarity(np.array(recognized), np.array(enrolled))\n",
    "print(\"Cosine similarity matrix\\n [Alice      Bob       ]\")\n",
    "print(sim_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca922ce",
   "metadata": {},
   "source": [
    "### Face Tracking Tutorial\n",
    "\n",
    "Define face tracking configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define configuration for face filtering\n",
    "face_filter_config = degirum_face.FaceFilterConfig(\n",
    "    enable_small_face_filter=True,\n",
    "    min_face_size=30,\n",
    "    enable_zone_filter=True,\n",
    "    zone=[[100, 10], [960, 10], [960, 700], [100, 700]],\n",
    "    enable_frontal_filter=True,\n",
    "    enable_shift_filter=True,\n",
    "    enable_reid_expiration_filter=True,\n",
    "    reid_expiration_frames=10,\n",
    ")\n",
    "\n",
    "# define configuration for clip storage (use local directory for tutorial, but can be any S3-compatible storage)\n",
    "clip_storage_config = degirum_tools.ObjectStorageConfig(\n",
    "    endpoint=\"./temp\", access_key=\"\", secret_key=\"\", bucket=\"tutorial_videos\"\n",
    ")\n",
    "\n",
    "# define face tracking configuration\n",
    "face_tracking_config = degirum_face.FaceTrackingConfig(\n",
    "    video_source=\"assets/WalkingPeople.mp4\",\n",
    "    face_detection_model=face_detection_model,\n",
    "    face_embedding_model=face_embedding_model,\n",
    "    db=db,\n",
    "    face_filter_config=face_filter_config,\n",
    "    clip_storage_config=clip_storage_config,\n",
    "    clip_duration=300,    \n",
    "    alert_mode=degirum_face.AlertMode.ON_UNKNOWNS,\n",
    "    credence_count=3,\n",
    ")\n",
    "\n",
    "# create clip manager instance\n",
    "clip_manager = degirum_face.FaceClipManager(face_tracking_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859e2cd",
   "metadata": {},
   "source": [
    "Run pipeline on database. We will collect video clips of unknown persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear all saved clips first\n",
    "clip_manager.remove_all_clips()\n",
    "\n",
    "# then run the face tracking pipeline\n",
    "composition, _ = degirum_face.start_face_tracking_pipeline(face_tracking_config)\n",
    "composition.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9c8a6",
   "metadata": {},
   "source": [
    "Analyze collected video clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all collected clips\n",
    "all_clips = clip_manager.list_clips()\n",
    "print(f\"Clips saved: {len(all_clips)}\")\n",
    "\n",
    "# show them by downloading from storage:\n",
    "for clip in all_clips:\n",
    "    video_display(clip_manager.download_clip(clip + \".mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a7b54c",
   "metadata": {},
   "source": [
    "Annotate collected video clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for clip in all_clips:\n",
    "    # annotate clip: it will return face map object indexed by object track IDs and containing face embeddings\n",
    "    face_map = clip_manager.find_faces_in_clip(clip)\n",
    "\n",
    "    # show annotated clip\n",
    "    video_display(\n",
    "        clip_manager.download_clip(clip + clip_manager.annotated_video_suffix)\n",
    "    )\n",
    "\n",
    "    break  # annotate only first clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee73bf9c",
   "metadata": {},
   "source": [
    "Add (enroll) all persons detected on the video clip to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see on a video above that object #1 is Bob and object #2 is Alice\n",
    "\n",
    "# Add embeddings for each object to the database\n",
    "face_tracking_config.db.add_embeddings_for_attributes(\"Bob\", face_map.map[1].embeddings)\n",
    "face_tracking_config.db.add_embeddings_for_attributes(\"Alice\", face_map.map[2].embeddings)\n",
    "\n",
    "# show number of enrolled embeddings in database\n",
    "print(face_tracking_config.db.count_embeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae915429",
   "metadata": {},
   "source": [
    "Run pipeline one more time: now should be no alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ec88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear all saved clips first\n",
    "clip_manager.remove_all_clips()\n",
    "\n",
    "# start face tracking pipeline\n",
    "composition, _ = degirum_face.start_face_tracking_pipeline(face_tracking_config)\n",
    "composition.wait()\n",
    "\n",
    "clips = clip_manager.list_clips()\n",
    "print(f\"Now we have: {len(clips)} clip(s)\")\n",
    "if not clips:\n",
    "    print(\"No alerts detected!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
